{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "MTYmjK3OEt4f",
      "metadata": {
        "id": "MTYmjK3OEt4f"
      },
      "source": [
        "# Prediction of Department: Fine-Tuning a Classification Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd3PW_CNDr2G",
      "metadata": {
        "id": "bd3PW_CNDr2G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, classification_report\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "from sklearn.utils import resample\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXKX7JBtEcWa",
      "metadata": {
        "id": "YXKX7JBtEcWa"
      },
      "outputs": [],
      "source": [
        "GH_USER = \"luisadosch\"\n",
        "GH_REPO = \"Final-Project-snapAddy\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "\n",
        "def get_github_url(relative_path):\n",
        "    return f\"https://raw.githubusercontent.com/{GH_USER}/{GH_REPO}/{BRANCH}/{relative_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aF-a2KOkGNfD",
      "metadata": {
        "id": "aF-a2KOkGNfD"
      },
      "outputs": [],
      "source": [
        "jobs_annotated = pd.read_csv(get_github_url(\"data/processed/jobs_annotated.csv\"))\n",
        "df_department = pd.read_csv(get_github_url(\"data/raw/department-v2.csv\"))\n",
        "\n",
        "# test set from annotated CVs (ACTIVE only) -> out-of-production set\n",
        "jobs_annotated_df = jobs_annotated.loc[\n",
        "    jobs_annotated[\"status\"] == \"ACTIVE\",\n",
        "    [\"position\", \"department\"]\n",
        "].copy()\n",
        "\n",
        "jobs_annotated_df = jobs_annotated_df.rename(columns={\"position\": \"text\", \"department\": \"label\"})\n",
        "\n",
        "# base train/test dfs (strings)\n",
        "df_department = df_department[[\"text\", \"label\"]].copy()\n",
        "jobs_annotated_df = jobs_annotated_df[[\"text\", \"label\"]].copy()\n",
        "\n",
        "print(\"fine tuning data:\", len(df_department), \"CV labeled data:\", len(jobs_annotated_df))\n",
        "print(\"fine-tune label counts:\\n\", df_department[\"label\"].value_counts())\n",
        "print(\"out-of-production label counts:\\n\", jobs_annotated_df[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_base, temp_df = train_test_split(\n",
        "    df_department,\n",
        "    test_size=0.30,              # 70% train, 30% temp\n",
        "    stratify=df_department[\"label\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "val_df_base, df_department_test = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.50,              # 15% val, 15% test\n",
        "    stratify=temp_df[\"label\"],\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(\"train:\", len(train_df_base), \"val:\", len(val_df_base), \"test:\", len(df_department_test))\n",
        "print(\"train label counts:\\n\", train_df_base[\"label\"].value_counts())\n",
        "print(\"val label counts:\\n\", val_df_base[\"label\"].value_counts())\n",
        "print(\"test label counts:\\n\", df_department_test[\"label\"].value_counts())"
      ],
      "metadata": {
        "id": "5UrvXgfAWNfg"
      },
      "id": "5UrvXgfAWNfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HL9DDmDLIbQd",
      "metadata": {
        "id": "HL9DDmDLIbQd"
      },
      "outputs": [],
      "source": [
        "def add_synthetic_department(train_df: pd.DataFrame, synthetic_csv_relpath: str) -> pd.DataFrame:\n",
        "    syn = pd.read_csv(get_github_url(synthetic_csv_relpath))\n",
        "\n",
        "    # expect columns: position, department\n",
        "    syn = syn[[\"position\", \"department\"]].copy()\n",
        "    syn = syn.rename(columns={\"position\": \"text\", \"department\": \"label\"})\n",
        "    syn = syn.dropna(subset=[\"text\", \"label\"])\n",
        "\n",
        "    out = pd.concat([train_df[[\"text\", \"label\"]], syn[[\"text\", \"label\"]]], ignore_index=True)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Frz7vJrbGtyz",
      "metadata": {
        "id": "Frz7vJrbGtyz"
      },
      "outputs": [],
      "source": [
        "MODEL_CKPT = \"xlm-roberta-base\"\n",
        "MAX_LEN = 80\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w8BcluBEGv0q",
      "metadata": {
        "id": "w8BcluBEGv0q"
      },
      "outputs": [],
      "source": [
        "def to_hf_dataset(df: pd.DataFrame, label_col: str = \"labels\") -> Dataset:\n",
        "    ds = Dataset.from_pandas(df[[\"text\", label_col]].copy(), preserve_index=False)\n",
        "    ds = ds.map(tokenize, batched=True)\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", label_col])\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dOFVScUGwkO",
      "metadata": {
        "id": "6dOFVScUGwkO"
      },
      "outputs": [],
      "source": [
        "def eval_split(trainer, train_ds, val_ds, test_ds):\n",
        "    train_metrics = trainer.evaluate(train_ds)\n",
        "    val_metrics   = trainer.evaluate(val_ds)\n",
        "    test_metrics  = trainer.evaluate(test_ds)\n",
        "    print(\"train:\", train_metrics)\n",
        "    print(\"val:  \", val_metrics)\n",
        "    print(\"test: \", test_metrics)\n",
        "    return train_metrics, val_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_curves(trainer):\n",
        "    logs = pd.DataFrame(trainer.state.log_history)\n",
        "\n",
        "    train_loss = logs.dropna(subset=[\"loss\"])[[\"step\", \"loss\"]].copy()\n",
        "    eval_loss  = logs.dropna(subset=[\"eval_loss\"])[[\"step\", \"eval_loss\"]].copy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(train_loss[\"step\"], train_loss[\"loss\"])\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"train loss\")\n",
        "    plt.title(\"Train loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(eval_loss[\"step\"], eval_loss[\"eval_loss\"])\n",
        "    plt.xlabel(\"step\")\n",
        "    plt.ylabel(\"eval loss\")\n",
        "    plt.title(\"Eval loss\")\n",
        "    plt.show()\n",
        "\n",
        "    return logs"
      ],
      "metadata": {
        "id": "HFWYvq8aWu0r"
      },
      "id": "HFWYvq8aWu0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9zbDCIfOG5cN",
      "metadata": {
        "id": "9zbDCIfOG5cN"
      },
      "source": [
        "classification pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fk_NX7e5G0rr",
      "metadata": {
        "id": "fk_NX7e5G0rr"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_cls(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "va_78dU-G849",
      "metadata": {
        "id": "va_78dU-G849"
      },
      "outputs": [],
      "source": [
        "def predict_table_cls(trainer, ds, df_text_label, id2label):\n",
        "    pred = trainer.predict(ds)\n",
        "    probs = pred.predictions\n",
        "    y_pred = np.argmax(probs, axis=-1)\n",
        "\n",
        "    out = df_text_label.copy().reset_index(drop=True)\n",
        "    out[\"pred_id\"] = y_pred\n",
        "    out[\"pred_label\"] = [id2label[int(i)] for i in y_pred]\n",
        "    out[\"correct\"] = (out[\"pred_label\"] == out[\"label\"])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lBe_RHF4HA2t",
      "metadata": {
        "id": "lBe_RHF4HA2t"
      },
      "outputs": [],
      "source": [
        "def report_cls(trainer, ds, id2label):\n",
        "    pred = trainer.predict(ds)\n",
        "    y_pred = np.argmax(pred.predictions, axis=-1)\n",
        "    y_true = pred.label_ids\n",
        "    print(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(len(id2label))]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def oversample_df(df, label_col=\"label\", random_state=42):\n",
        "    max_n = df[label_col].value_counts().max()\n",
        "    parts = []\n",
        "    for lab, g in df.groupby(label_col):\n",
        "        parts.append(resample(g, replace=True, n_samples=max_n, random_state=random_state))\n",
        "    return pd.concat(parts).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def run_classification(\n",
        "    train_df,\n",
        "    val_df,\n",
        "    test_df,\n",
        "    out_dir=\"dept_cls\",\n",
        "    do_oversample=False,\n",
        "    random_state=42,\n",
        "):\n",
        "    # label space ONLY from train_df (no leakage)\n",
        "    label_list = sorted(train_df[\"label\"].unique())\n",
        "    label2id = {l: i for i, l in enumerate(label_list)}\n",
        "    id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "    tr = train_df.copy()\n",
        "    va = val_df.copy()\n",
        "    te = test_df.copy()\n",
        "\n",
        "    tr[\"labels\"] = tr[\"label\"].map(label2id).astype(int)\n",
        "    va[\"labels\"] = va[\"label\"].map(label2id)\n",
        "    te[\"labels\"] = te[\"label\"].map(label2id)\n",
        "\n",
        "    # oversample train only (optional)\n",
        "    train_os = tr\n",
        "    if do_oversample:\n",
        "        train_os = oversample_df(tr, label_col=\"label\", random_state=random_state)\n",
        "\n",
        "    train_ds = to_hf_dataset(train_os, label_col=\"labels\")\n",
        "    val_ds = to_hf_dataset(va, label_col=\"labels\")\n",
        "    test_ds = to_hf_dataset(te, label_col=\"labels\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_CKPT,\n",
        "        num_labels=len(label2id),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        learning_rate=1e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.05,\n",
        "        warmup_ratio=0.06,\n",
        "        do_eval=True,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=False,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "        dataloader_num_workers=0,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=50,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics_cls,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    eval_split(trainer, train_ds, val_ds, test_ds)\n",
        "    loss_curves(trainer)\n",
        "\n",
        "    train_pred = predict_table_cls(trainer, train_ds, train_os[[\"text\", \"label\"]], id2label)\n",
        "    val_pred = predict_table_cls(trainer, val_ds, va[[\"text\", \"label\"]], id2label)\n",
        "    test_pred = predict_table_cls(trainer, test_ds, te[[\"text\", \"label\"]], id2label)\n",
        "\n",
        "    report_cls(trainer, val_ds, id2label)\n",
        "\n",
        "    return trainer, (train_pred, val_pred, test_pred), (label2id, id2label)\n"
      ],
      "metadata": {
        "id": "O0PwPIRLb2lj"
      },
      "id": "O0PwPIRLb2lj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "zt5cxxjvHIJl",
      "metadata": {
        "id": "zt5cxxjvHIJl"
      },
      "source": [
        "model runs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MfhZ42aAHRfG",
      "metadata": {
        "id": "MfhZ42aAHRfG"
      },
      "source": [
        "classification without synth. data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jb1KdmKNHMFm",
      "metadata": {
        "id": "jb1KdmKNHMFm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"cuda device count:\", torch.cuda.device_count())\n",
        "print(\"current device:\", torch.cuda.current_device() if torch.cuda.is_available() else None)\n",
        "print(\"gpu name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BopXYHPdHTuP",
      "metadata": {
        "id": "BopXYHPdHTuP"
      },
      "source": [
        "classification with synthetic data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MVaI8X9_Js9n",
      "metadata": {
        "id": "MVaI8X9_Js9n"
      },
      "source": [
        "1) Normal (no synthetic, no oversampling)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept_trainer_base, (dept_train_pred, dept_val_pred, dept_test_pred), (label2id_dept, id2label_dept) = run_classification(\n",
        "    train_df_base,\n",
        "    val_df_base,\n",
        "    df_department_test,\n",
        "    out_dir=\"department_cls_base\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "taLJsBMFcAJ0"
      },
      "id": "taLJsBMFcAJ0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dept = jobs_annotated_df[[\"text\", \"label\"]].copy()\n",
        "cv_dept[\"labels\"] = cv_dept[\"label\"].map(label2id_dept).astype(int)\n",
        "\n",
        "cv_dept_ds = to_hf_dataset(cv_dept, label_col=\"labels\")\n",
        "\n",
        "cv_dept_pred = predict_table_cls(\n",
        "    dept_trainer_base,\n",
        "    cv_dept_ds,\n",
        "    cv_dept[[\"text\", \"label\"]],\n",
        "    id2label_dept\n",
        ")\n",
        "\n",
        "print(\"CV accuracy:\", cv_dept_pred[\"correct\"].mean())\n",
        "print(\"CV F1 macro:\", f1_score(cv_dept_pred[\"label\"], cv_dept_pred[\"pred_label\"], average=\"macro\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "HCsNxte2nAeV"
      },
      "id": "HCsNxte2nAeV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "mSHg7srjJtxo",
      "metadata": {
        "id": "mSHg7srjJtxo"
      },
      "source": [
        "2) No synthetic + oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L-gLrZstJo0v",
      "metadata": {
        "id": "L-gLrZstJo0v"
      },
      "outputs": [],
      "source": [
        "dept_trainer_os, (dept_train_pred, dept_val_pred, dept_test_pred), (label2id_dept, id2label_dept) = run_classification(\n",
        "    train_df_base,\n",
        "    val_df_base,\n",
        "    df_department_test,\n",
        "    out_dir=\"department_cls_os\",\n",
        "    do_oversample=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dept_pred_os = predict_table_cls(\n",
        "    dept_trainer_os,\n",
        "    cv_dept_ds,\n",
        "    cv_dept[[\"text\", \"label\"]],\n",
        "    id2label_dept\n",
        ")\n",
        "\n",
        "print(\"CV accuracy:\", cv_dept_pred_os[\"correct\"].mean())\n",
        "print(\"CV F1 macro:\", f1_score(cv_dept_pred_os[\"label\"], cv_dept_pred_os[\"pred_label\"], average=\"macro\"))"
      ],
      "metadata": {
        "id": "0bzaUTn6sNMc"
      },
      "id": "0bzaUTn6sNMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "yriXru__QX7V",
      "metadata": {
        "id": "yriXru__QX7V"
      },
      "source": [
        "## 3) With synthetic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_aug = add_synthetic_department(train_df_base, \"data/results/gemini_synthetic.csv\")"
      ],
      "metadata": {
        "id": "6v3ZmacpvVjU"
      },
      "id": "6v3ZmacpvVjU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show how often the different labels occur in train_df_aug\n",
        "\n",
        "train_df_aug[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "1fs9tgU2vYyA"
      },
      "id": "1fs9tgU2vYyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dept_trainer_aug, (dept_train_pred_aug, dept_val_pred_aug, dept_test_pred_aug), (label2id_dept, id2label_dept) = run_classification(\n",
        "    train_df_aug,\n",
        "    val_df_base,\n",
        "    df_department_test,\n",
        "    out_dir=\"department_cls_aug\",\n",
        "    do_oversample=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "yqr-UTRPvk4h"
      },
      "id": "yqr-UTRPvk4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dept_ds = to_hf_dataset(cv_dept, label_col=\"labels\")\n",
        "dept_trainer_aug.evaluate(cv_dept_ds)"
      ],
      "metadata": {
        "id": "HG5tWV6YME0L"
      },
      "id": "HG5tWV6YME0L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dept_pred_aug"
      ],
      "metadata": {
        "id": "ff7Ro99uKCfI"
      },
      "id": "ff7Ro99uKCfI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}