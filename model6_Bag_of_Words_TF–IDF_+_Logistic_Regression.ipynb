{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFcEtEVtU6RIJKYF8PAs5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisadosch/Final-Project-snapAddy/blob/main/model6_Bag_of_Words_TF%E2%80%93IDF_%2B_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Github-Zugangsdaten"
      ],
      "metadata": {
        "id": "iToNPrLm7dwJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8Lb-5hvvaCul"
      },
      "outputs": [],
      "source": [
        "# GitHub-Zugangsdaten\n",
        "import pandas as pd\n",
        "\n",
        "GH_USER = \"luisadosch\"\n",
        "GH_REPO = \"Final-Project-snapAddy\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "def get_github_url(relative_path):\n",
        "    return f\"https://raw.githubusercontent.com/{GH_USER}/{GH_REPO}/{BRANCH}/{relative_path}\"\n",
        "\n",
        "\n",
        "jobs_annotated_active_df = pd.read_csv(get_github_url(\"data/processed/jobs_annotated_active.csv\"))\n",
        "\n",
        "department_df = pd.read_csv(get_github_url(\"data/raw/department-v2.csv\"))\n",
        "\n",
        "seniority_df = pd.read_csv(get_github_url(\"data/raw/seniority-v2.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Modell Seniority"
      ],
      "metadata": {
        "id": "lf3ury7l7WS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority Daten sortieren\n",
        "sdf = seniority_df.copy()\n",
        "\n",
        "sdf[\"text\"] = sdf[\"text\"].astype(str).str.lower()\n",
        "sdf[\"label\"] = sdf[\"label\"].astype(str)\n",
        "\n",
        "sdf = sdf.dropna(subset=[\"text\", \"label\"])"
      ],
      "metadata": {
        "id": "myWrrnu33LQh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the seniority dataset for modeling. Lowercasing ensures uniform text representation. Dropping missing values prevents errors in the model."
      ],
      "metadata": {
        "id": "o0AJ6UU6_zim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority Train/Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sx = sdf[\"text\"]\n",
        "sy = sdf[\"label\"]\n",
        "\n",
        "sx_train, sx_test, sy_train, sy_test = train_test_split(\n",
        "    sx,\n",
        "    sy,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=sy\n",
        ")\n",
        "\n",
        "# Print dataset sizes\n",
        "print(\"Seniority dataset sizes:\")\n",
        "print(\"Total:\", len(sx))\n",
        "print(\"Train:\", len(sx_train))\n",
        "print(\"Test:\", len(sx_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZhuiHe25MUC",
        "outputId": "7a2f2db6-8fa1-4146-80e5-8048facf8c5d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seniority dataset sizes:\n",
            "Total: 9428\n",
            "Train: 7542\n",
            "Test: 1886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into training and test sets. stratify ensures rare classes are represented proportionally. The total is 9428, while the train set is 7542 and the test set is 1886."
      ],
      "metadata": {
        "id": "npomIeP4_9ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority TF–IDF + Logistic Regression Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "smodel = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        ngram_range=(1, 2),   # unigrams + bigrams\n",
        "        min_df=3,\n",
        "        max_df=0.9\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\"\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "No0ZoKxQ548R"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline converts job titles into TF–IDF features and applies logistic regression. class_weight=\"balanced\" ensures rare seniority levels get enough importance."
      ],
      "metadata": {
        "id": "JCKm5XSqAA9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority Modell trainieren\n",
        "smodel.fit(sx_train, sy_train)\n",
        "\n",
        "# Vorhersagen auf Testdaten\n",
        "sy_pred = smodel.predict(sx_test)\n",
        "\n",
        "# Accuracy ausgeben\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\", accuracy_score(sy_test, sy_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP5-HhQJ6AXL",
        "outputId": "c863fd88-e888-4219-fd9d-0ba681f2846e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9703075291622482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the seniority classifier and generate predictions on the test set. The model achieves a high accuracy of 0.97 on the test set."
      ],
      "metadata": {
        "id": "bQ9qxNL8AedP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority Evaluation\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "sy_pred = smodel.predict(sx_test)\n",
        "\n",
        "saccuracy = accuracy_score(sy_test, sy_pred)\n",
        "smacro_f1 = f1_score(sy_test, sy_pred, average=\"macro\")\n",
        "\n",
        "print(\"Accuracy:\", round(saccuracy, 3))\n",
        "print(\"Macro F1:\", round(smacro_f1, 3))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(sy_test, sy_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0p5PPSC6jJO",
        "outputId": "e592ea6c-ef73-4830-8760-ceacde1d3982"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.97\n",
            "Macro F1: 0.956\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Director       0.99      0.98      0.98       197\n",
            "      Junior       0.85      1.00      0.92        82\n",
            "        Lead       0.97      0.98      0.98       709\n",
            "  Management       0.92      0.93      0.92       151\n",
            "      Senior       0.99      0.97      0.98       747\n",
            "\n",
            "    accuracy                           0.97      1886\n",
            "   macro avg       0.94      0.97      0.96      1886\n",
            "weighted avg       0.97      0.97      0.97      1886\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate using accuracy and macro F1, which accounts for class imbalance. Classification report shows precision, recall, and F1 per seniority level. The evaluation yields an accuracy of 0.97 and a macro F1 score of 0.956, reflecting strong performance across all seniority classes."
      ],
      "metadata": {
        "id": "4nK-QA8JB_e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seniority Evaluation on Annotated ACTIVE Jobs\n",
        "\n",
        "# Prepare evaluation data\n",
        "s_eval_df = jobs_annotated_active_df.dropna(subset=[\"position\", \"seniority\"]).copy()\n",
        "s_eval_text = s_eval_df[\"position\"].astype(str).str.lower()\n",
        "s_eval_labels = s_eval_df[\"seniority\"].astype(str)\n",
        "\n",
        "# Predict seniority\n",
        "s_eval_pred = smodel.predict(s_eval_text)\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "s_eval_accuracy = accuracy_score(s_eval_labels, s_eval_pred)\n",
        "s_eval_macro_f1 = f1_score(s_eval_labels, s_eval_pred, average=\"macro\")\n",
        "\n",
        "print(\"Seniority Evaluation on ACTIVE Jobs\")\n",
        "print(\"Accuracy:\", round(s_eval_accuracy, 3))\n",
        "print(\"Macro F1:\", round(s_eval_macro_f1, 3))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(s_eval_labels, s_eval_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUSJTiDGkxfb",
        "outputId": "ed90e530-8925-4722-a653-0d25263e9a5c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seniority Evaluation on ACTIVE Jobs\n",
            "Accuracy: 0.437\n",
            "Macro F1: 0.409\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Director       0.58      0.88      0.70        34\n",
            "      Junior       0.18      0.33      0.24        12\n",
            "        Lead       0.33      0.71      0.45       125\n",
            "  Management       0.90      0.59      0.72       192\n",
            "Professional       0.00      0.00      0.00       216\n",
            "      Senior       0.23      0.80      0.36        44\n",
            "\n",
            "    accuracy                           0.44       623\n",
            "   macro avg       0.37      0.55      0.41       623\n",
            "weighted avg       0.40      0.44      0.38       623\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluates the seniority model on annotated ACTIVE LinkedIn job entries using the job position title as input. The predictions are compared against manually labeled seniority levels, providing a realistic assessment of how well the model generalizes from label-based training data to real-world CV data. The accuracy ist 0.437 and the macro F1 score is 0.409."
      ],
      "metadata": {
        "id": "BKgo9KBA8LB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Seniority Top Features\n",
        "import numpy as np\n",
        "\n",
        "feature_names = smodel.named_steps[\"tfidf\"].get_feature_names_out()\n",
        "coefs = smodel.named_steps[\"clf\"].coef_\n",
        "\n",
        "for i, label in enumerate(smodel.named_steps[\"clf\"].classes_):\n",
        "    top = np.argsort(coefs[i])[-10:]\n",
        "    print(f\"\\nTop words for {label}:\")\n",
        "    print(feature_names[top])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfFqcaei6ob3",
        "outputId": "73b60c09-bef4-4b86-d70d-a7740eeac6d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top words for Director:\n",
            "['marketing director' 'managing directors' 'managing' 'director marketing'\n",
            " 'vertriebsdirektor' 'director sales' 'directors' 'sales director'\n",
            " 'abteilungsdirektor' 'director']\n",
            "\n",
            "Top words for Junior:\n",
            "['marketing' 'assistent' 'associate' 'assistentin' 'mitarbeiter'\n",
            " 'mitarbeiterin' 'referent' 'referentin' 'analyst' 'junior']\n",
            "\n",
            "Top words for Lead:\n",
            "['abteilungsleiter' 'projektleiter' 'geschäftsleitung' 'teamleiter'\n",
            " 'leiterin' 'head of' 'head' 'vertriebsleiter' 'leitung' 'leiter']\n",
            "\n",
            "Top words for Management:\n",
            "['cio' 'vice president' 'vice' 'founder' 'chief' 'owner' 'ceo'\n",
            " 'geschäftsführung' 'vp' 'geschäftsführer']\n",
            "\n",
            "Top words for Senior:\n",
            "['marketing manager' 'engineer' 'executive' 'assistant' 'managerin'\n",
            " 'responsable' 'consultant' 'management' 'senior' 'manager']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the most influential words for predicting each seniority class. Helps interpret the model’s decisions."
      ],
      "metadata": {
        "id": "6gshwmgwCheN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Modell Department"
      ],
      "metadata": {
        "id": "5dWHOcEI7m2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Department Daten sortieren\n",
        "ddf = department_df.copy()\n",
        "\n",
        "ddf[\"text\"] = ddf[\"text\"].astype(str).str.lower()\n",
        "ddf[\"label\"] = ddf[\"label\"].astype(str)\n",
        "\n",
        "ddf = ddf.dropna(subset=[\"text\", \"label\"])"
      ],
      "metadata": {
        "id": "2VDI66G17o79"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare department dataset. Lowercasing ensures consistent text representation. Drop missing values."
      ],
      "metadata": {
        "id": "mmgYI0xv52UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department Train/Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dx = ddf[\"text\"]\n",
        "dy = ddf[\"label\"]\n",
        "\n",
        "dx_train, dx_test, dy_train, dy_test = train_test_split(\n",
        "    dx,\n",
        "    dy,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=dy\n",
        ")\n",
        "\n",
        "# Print dataset sizes\n",
        "print(\"Department dataset sizes:\")\n",
        "print(\"Total:\", len(dx))\n",
        "print(\"Train:\", len(dx_train))\n",
        "print(\"Test:\", len(dx_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUsWdTaa8c2R",
        "outputId": "51ce97b2-064c-4ad8-fcbf-242d5332e5ff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Department dataset sizes:\n",
            "Total: 10145\n",
            "Train: 8116\n",
            "Test: 2029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train/test sets with proportional label distribution. The total is 10145, while the train set is 8116 and the test set is 2029."
      ],
      "metadata": {
        "id": "q6BpltzS544-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department TF–IDF + Logistic Regression Pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "dmodel = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(\n",
        "        ngram_range=(1, 2),   # unigrams + bigrams\n",
        "        min_df=3,\n",
        "        max_df=0.9\n",
        "    )),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight=\"balanced\"\n",
        "    ))\n",
        "])"
      ],
      "metadata": {
        "id": "Zl7yJL7p-Vh9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as seniority pipeline but for department prediction."
      ],
      "metadata": {
        "id": "QYDOM_m0565H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department Modell trainieren\n",
        "dmodel.fit(dx_train, dy_train)\n",
        "\n",
        "# Vorhersagen auf Testdaten\n",
        "dy_pred = dmodel.predict(dx_test)\n",
        "\n",
        "# Accuracy ausgeben\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\", accuracy_score(dy_test, dy_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wyME-xP-g8j",
        "outputId": "007c260f-6d23-4054-c05d-3a3d4cdcbe82"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9344504682109414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the department classifier and predict on test set. The model achieves a high accuracy of 0.93 in the test set."
      ],
      "metadata": {
        "id": "EkOGOOJR5-Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department Evaluation\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "dy_pred = dmodel.predict(dx_test)\n",
        "\n",
        "daccuracy = accuracy_score(dy_test, dy_pred)\n",
        "dmacro_f1 = f1_score(dy_test, dy_pred, average=\"macro\")\n",
        "\n",
        "print(\"Accuracy:\", round(daccuracy, 3))\n",
        "print(\"Macro F1:\", round(dmacro_f1, 3))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(dy_test, dy_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp4orw2W-kE2",
        "outputId": "767b50ba-bae8-46ba-d569-8129211cf16e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.934\n",
            "Macro F1: 0.86\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "        Administrative       0.62      0.94      0.74        17\n",
            "  Business Development       0.83      0.99      0.90       124\n",
            "            Consulting       0.82      0.97      0.89        33\n",
            "      Customer Support       0.88      1.00      0.93         7\n",
            "       Human Resources       0.75      1.00      0.86         6\n",
            "Information Technology       0.92      0.95      0.94       261\n",
            "             Marketing       0.99      0.92      0.96       859\n",
            "                 Other       0.50      1.00      0.67         8\n",
            "    Project Management       0.57      0.88      0.69        40\n",
            "            Purchasing       0.89      1.00      0.94         8\n",
            "                 Sales       0.96      0.93      0.94       666\n",
            "\n",
            "              accuracy                           0.93      2029\n",
            "             macro avg       0.79      0.96      0.86      2029\n",
            "          weighted avg       0.95      0.93      0.94      2029\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and macro F1 score for department model. Classification report for detailed class-level metrics.\n",
        "Evaluate using accuracy and macro F1, which accounts for class imbalance. Classification report shows precision, recall, and F1 per department level. The evaluation yields an accuracy of 0.934 and a macro F1 score of 0.86, reflecting strong performance across all department classes."
      ],
      "metadata": {
        "id": "UMFRnIgc6PV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department Evaluation on Annotated ACTIVE Jobs\n",
        "\n",
        "# Prepare evaluation data\n",
        "d_eval_df = jobs_annotated_active_df.dropna(subset=[\"position\", \"department\"]).copy()\n",
        "d_eval_text = d_eval_df[\"position\"].astype(str).str.lower()\n",
        "d_eval_labels = d_eval_df[\"department\"].astype(str)\n",
        "\n",
        "# Predict department\n",
        "d_eval_pred = dmodel.predict(d_eval_text)\n",
        "\n",
        "# Evaluation metrics\n",
        "d_eval_accuracy = accuracy_score(d_eval_labels, d_eval_pred)\n",
        "d_eval_macro_f1 = f1_score(d_eval_labels, d_eval_pred, average=\"macro\")\n",
        "\n",
        "print(\"Department Evaluation on ACTIVE Jobs\")\n",
        "print(\"Accuracy:\", round(d_eval_accuracy, 3))\n",
        "print(\"Macro F1:\", round(d_eval_macro_f1, 3))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(d_eval_labels, d_eval_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OAez4n26hEM",
        "outputId": "dd95f587-f0d4-4642-fb8a-c4d1b335dd87"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Department Evaluation on ACTIVE Jobs\n",
            "Accuracy: 0.223\n",
            "Macro F1: 0.338\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "                        precision    recall  f1-score   support\n",
            "\n",
            "        Administrative       0.17      0.07      0.10        14\n",
            "  Business Development       0.38      0.30      0.33        20\n",
            "            Consulting       0.86      0.46      0.60        39\n",
            "      Customer Support       1.00      0.17      0.29         6\n",
            "       Human Resources       0.73      0.50      0.59        16\n",
            "Information Technology       0.31      0.44      0.36        62\n",
            "             Marketing       0.17      0.41      0.24        22\n",
            "                 Other       0.00      0.00      0.00       344\n",
            "    Project Management       0.27      0.56      0.37        39\n",
            "            Purchasing       0.80      0.53      0.64        15\n",
            "                 Sales       0.12      0.85      0.21        46\n",
            "\n",
            "              accuracy                           0.22       623\n",
            "             macro avg       0.44      0.39      0.34       623\n",
            "          weighted avg       0.18      0.22      0.17       623\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluates the department model on annotated ACTIVE LinkedIn job entries based on the job position title. Evaluating on manually labeled CV data allows for assessing the model’s robustness and applicability in realistic LinkedIn profile scenarios. The accuracy is 0.223 and the macro F1 score is 0.338."
      ],
      "metadata": {
        "id": "m_j5zHRf9td3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Department Top Features\n",
        "import numpy as np\n",
        "\n",
        "feature_names = dmodel.named_steps[\"tfidf\"].get_feature_names_out()\n",
        "coefs = dmodel.named_steps[\"clf\"].coef_\n",
        "\n",
        "for i, label in enumerate(dmodel.named_steps[\"clf\"].classes_):\n",
        "    top = np.argsort(coefs[i])[-10:]\n",
        "    print(f\"\\nTop words for {label}:\")\n",
        "    print(feature_names[top])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBieqTW0-ufz",
        "outputId": "57fea286-7fcf-479f-f127-2d73d6f470b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top words for Administrative:\n",
            "['assistentin des' 'geschäftsführung' 'gf' 'assistent der'\n",
            " 'geschäftsleitung' 'der' 'sekretärin' 'assistent' 'assistenz'\n",
            " 'assistentin']\n",
            "\n",
            "Top words for Business Development:\n",
            "['business intelligence' 'crm' 'digital business' 'of business'\n",
            " 'business process' 'ebusiness' 'it business' 'development'\n",
            " 'business development' 'business']\n",
            "\n",
            "Top words for Consulting:\n",
            "['senior berater' 'sap' 'coach' 'senior consultant' 'von' 'senior'\n",
            " 'recruitment' 'beraterin' 'berater' 'consultant']\n",
            "\n",
            "Top words for Customer Support:\n",
            "['service and' 'it systems' 'customer' 'technical' 'it support' 'it'\n",
            " 'customer support' 'technical support' 'supporter' 'support']\n",
            "\n",
            "Top words for Human Resources:\n",
            "['qualitätsmanagement' 'director digital' 'project director' 'gl'\n",
            " 'manager hr' 'of human' 'resources' 'human resources' 'human' 'hr']\n",
            "\n",
            "Top words for Information Technology:\n",
            "['digitalization' 'administrator' 'entwickler' 'digitale' 'administration'\n",
            " 'digitalisierung' 'sap' 'digital' 'it' 'crm']\n",
            "\n",
            "Top words for Marketing:\n",
            "['marketing und' 'marketing sales' 'marketing manager' 'messen'\n",
            " 'sales marketing' 'kommunikation' 'messe' 'communications'\n",
            " 'communication' 'marketing']\n",
            "\n",
            "Top words for Other:\n",
            "['transformation and' 'customer management' 'of commercial'\n",
            " 'senior director' 'operations officer' 'chief operations' 'of'\n",
            " 'operations manager' 'of operations' 'operations']\n",
            "\n",
            "Top words for Project Management:\n",
            "['projektingenieur' 'projects' 'projekt' 'projektmanagerin' 'projekte'\n",
            " 'projektleitung' 'projektmanager' 'projektmanagement' 'projektleiter'\n",
            " 'project']\n",
            "\n",
            "Top words for Purchasing:\n",
            "['strategische' 'einkauf und' 'and global' 'und it' 'operative'\n",
            " 'leiter einkauf' 'einkäuferin' 'einkäufer' 'purchasing' 'einkauf']\n",
            "\n",
            "Top words for Sales:\n",
            "['account' 'sales manager' 'of sales' 'vertriebsinnendienst'\n",
            " 'vertriebsleitung' 'account manager' 'vertriebsleiter' 'salesforce'\n",
            " 'vertrieb' 'sales']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shows the top words for each department class to interpret the model."
      ],
      "metadata": {
        "id": "VoCRWEL36owr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vergleiche Accuracy und Macro F1\n",
        "comparison_metrics = pd.DataFrame({\n",
        "    \"Target\": [\"Seniority\", \"Department\"],\n",
        "    \"Accuracy\": [saccuracy, daccuracy],\n",
        "    \"Macro F1\": [smacro_f1, dmacro_f1]\n",
        "})\n",
        "\n",
        "print(\"Modellvergleich:\\n\")\n",
        "print(comparison_metrics)\n",
        "\n",
        "# Optional: Top 5 Features pro Label für beide Modelle nebeneinander\n",
        "def print_top_features(model, n=5):\n",
        "    feature_names = model.named_steps[\"tfidf\"].get_feature_names_out()\n",
        "    coefs = model.named_steps[\"clf\"].coef_\n",
        "    for i, label in enumerate(model.named_steps[\"clf\"].classes_):\n",
        "        top = np.argsort(coefs[i])[-n:]\n",
        "        print(f\"\\nTop {n} words for {label}:\")\n",
        "        print(feature_names[top])\n",
        "\n",
        "print(\"\\n--- Seniority Top Features ---\")\n",
        "print_top_features(smodel)\n",
        "\n",
        "print(\"\\n--- Department Top Features ---\")\n",
        "print_top_features(dmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH84vm9O_IIx",
        "outputId": "041f5b3e-cfb6-4d7e-c313-913207448876"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modellvergleich:\n",
            "\n",
            "       Target  Accuracy  Macro F1\n",
            "0   Seniority  0.970308  0.956030\n",
            "1  Department  0.934450  0.860444\n",
            "\n",
            "--- Seniority Top Features ---\n",
            "\n",
            "Top 5 words for Director:\n",
            "['director sales' 'directors' 'sales director' 'abteilungsdirektor'\n",
            " 'director']\n",
            "\n",
            "Top 5 words for Junior:\n",
            "['mitarbeiterin' 'referent' 'referentin' 'analyst' 'junior']\n",
            "\n",
            "Top 5 words for Lead:\n",
            "['head of' 'head' 'vertriebsleiter' 'leitung' 'leiter']\n",
            "\n",
            "Top 5 words for Management:\n",
            "['owner' 'ceo' 'geschäftsführung' 'vp' 'geschäftsführer']\n",
            "\n",
            "Top 5 words for Senior:\n",
            "['responsable' 'consultant' 'management' 'senior' 'manager']\n",
            "\n",
            "--- Department Top Features ---\n",
            "\n",
            "Top 5 words for Administrative:\n",
            "['der' 'sekretärin' 'assistent' 'assistenz' 'assistentin']\n",
            "\n",
            "Top 5 words for Business Development:\n",
            "['ebusiness' 'it business' 'development' 'business development' 'business']\n",
            "\n",
            "Top 5 words for Consulting:\n",
            "['senior' 'recruitment' 'beraterin' 'berater' 'consultant']\n",
            "\n",
            "Top 5 words for Customer Support:\n",
            "['it' 'customer support' 'technical support' 'supporter' 'support']\n",
            "\n",
            "Top 5 words for Human Resources:\n",
            "['of human' 'resources' 'human resources' 'human' 'hr']\n",
            "\n",
            "Top 5 words for Information Technology:\n",
            "['digitalisierung' 'sap' 'digital' 'it' 'crm']\n",
            "\n",
            "Top 5 words for Marketing:\n",
            "['kommunikation' 'messe' 'communications' 'communication' 'marketing']\n",
            "\n",
            "Top 5 words for Other:\n",
            "['chief operations' 'of' 'operations manager' 'of operations' 'operations']\n",
            "\n",
            "Top 5 words for Project Management:\n",
            "['projektleitung' 'projektmanager' 'projektmanagement' 'projektleiter'\n",
            " 'project']\n",
            "\n",
            "Top 5 words for Purchasing:\n",
            "['leiter einkauf' 'einkäuferin' 'einkäufer' 'purchasing' 'einkauf']\n",
            "\n",
            "Top 5 words for Sales:\n",
            "['account manager' 'vertriebsleiter' 'salesforce' 'vertrieb' 'sales']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison: Training Evaluation vs ACTIVE Job Evaluation\n",
        "\n",
        "comparison_metrics = pd.DataFrame({\n",
        "    \"Target\": [\n",
        "        \"Seniority (Label Data)\",\n",
        "        \"Department (Label Data)\",\n",
        "        \"Seniority (ACTIVE Jobs)\",\n",
        "        \"Department (ACTIVE Jobs)\"\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        saccuracy,\n",
        "        daccuracy,\n",
        "        s_eval_accuracy,\n",
        "        d_eval_accuracy\n",
        "    ],\n",
        "    \"Macro F1\": [\n",
        "        smacro_f1,\n",
        "        dmacro_f1,\n",
        "        s_eval_macro_f1,\n",
        "        d_eval_macro_f1\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\\n\")\n",
        "print(comparison_metrics)\n",
        "\n",
        "\n",
        "#Top Features per Label\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def print_top_features(model, n=5):\n",
        "    feature_names = model.named_steps[\"tfidf\"].get_feature_names_out()\n",
        "    coefs = model.named_steps[\"clf\"].coef_\n",
        "    for i, label in enumerate(model.named_steps[\"clf\"].classes_):\n",
        "        top = np.argsort(coefs[i])[-n:]\n",
        "        print(f\"\\nTop {n} words for {label}:\")\n",
        "        print(feature_names[top])\n",
        "\n",
        "print(\"\\n--- Seniority Top Features ---\")\n",
        "print_top_features(smodel)\n",
        "\n",
        "print(\"\\n--- Department Top Features ---\")\n",
        "print_top_features(dmodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuhyc0nr-IJ9",
        "outputId": "b6ad61d7-7973-4b81-86a4-2fbd6b91de38"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Comparison:\n",
            "\n",
            "                     Target  Accuracy  Macro F1\n",
            "0    Seniority (Label Data)  0.970308  0.956030\n",
            "1   Department (Label Data)  0.934450  0.860444\n",
            "2   Seniority (ACTIVE Jobs)  0.436597  0.409319\n",
            "3  Department (ACTIVE Jobs)  0.223114  0.338219\n",
            "\n",
            "--- Seniority Top Features ---\n",
            "\n",
            "Top 5 words for Director:\n",
            "['director sales' 'directors' 'sales director' 'abteilungsdirektor'\n",
            " 'director']\n",
            "\n",
            "Top 5 words for Junior:\n",
            "['mitarbeiterin' 'referent' 'referentin' 'analyst' 'junior']\n",
            "\n",
            "Top 5 words for Lead:\n",
            "['head of' 'head' 'vertriebsleiter' 'leitung' 'leiter']\n",
            "\n",
            "Top 5 words for Management:\n",
            "['owner' 'ceo' 'geschäftsführung' 'vp' 'geschäftsführer']\n",
            "\n",
            "Top 5 words for Senior:\n",
            "['responsable' 'consultant' 'management' 'senior' 'manager']\n",
            "\n",
            "--- Department Top Features ---\n",
            "\n",
            "Top 5 words for Administrative:\n",
            "['der' 'sekretärin' 'assistent' 'assistenz' 'assistentin']\n",
            "\n",
            "Top 5 words for Business Development:\n",
            "['ebusiness' 'it business' 'development' 'business development' 'business']\n",
            "\n",
            "Top 5 words for Consulting:\n",
            "['senior' 'recruitment' 'beraterin' 'berater' 'consultant']\n",
            "\n",
            "Top 5 words for Customer Support:\n",
            "['it' 'customer support' 'technical support' 'supporter' 'support']\n",
            "\n",
            "Top 5 words for Human Resources:\n",
            "['of human' 'resources' 'human resources' 'human' 'hr']\n",
            "\n",
            "Top 5 words for Information Technology:\n",
            "['digitalisierung' 'sap' 'digital' 'it' 'crm']\n",
            "\n",
            "Top 5 words for Marketing:\n",
            "['kommunikation' 'messe' 'communications' 'communication' 'marketing']\n",
            "\n",
            "Top 5 words for Other:\n",
            "['chief operations' 'of' 'operations manager' 'of operations' 'operations']\n",
            "\n",
            "Top 5 words for Project Management:\n",
            "['projektleitung' 'projektmanager' 'projektmanagement' 'projektleiter'\n",
            " 'project']\n",
            "\n",
            "Top 5 words for Purchasing:\n",
            "['leiter einkauf' 'einkäuferin' 'einkäufer' 'purchasing' 'einkauf']\n",
            "\n",
            "Top 5 words for Sales:\n",
            "['account manager' 'vertriebsleiter' 'salesforce' 'vertrieb' 'sales']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison table summarizes model performance across two evaluation settings with concrete quantitative results. On the label-based datasets, the seniority classifier achieves a very high accuracy of 0.97 with a macro F1 score of 0.96, while the department classifier reaches an accuracy of 0.93 and a macro F1 score of 0.86. These results indicate that both TF–IDF + logistic regression models perform extremely well when trained and evaluated on curated label data.\n",
        "\n",
        "When evaluated on annotated ACTIVE job entries from real CV data, performance drops substantially. Seniority prediction achieves an accuracy of 0.44 and a macro F1 score of 0.41, while department prediction performs considerably worse with an accuracy of 0.22 and a macro F1 score of 0.34. This sharp decline highlights a strong domain shift between clean label data and real-world job titles, which are shorter, noisier, more ambiguous, and often lack explicit domain or seniority cues.\n",
        "\n",
        "The observed performance gap confirms that while simple bag-of-words baselines are effective on controlled datasets, they struggle to generalize to realistic CV data."
      ],
      "metadata": {
        "id": "BsVupw7b63Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "results = []\n",
        "\n",
        "# ACTIVE jobs – Seniority\n",
        "add_result(\n",
        "    results,\n",
        "    model_name=\"bag-of-words/TF-IDF + Logistic Regression\",\n",
        "    target=\"Seniority (ACTIVE Jobs)\",\n",
        "    metrics={\n",
        "        \"accuracy\": s_eval_accuracy,\n",
        "        \"macro_f1\": s_eval_macro_f1\n",
        "    }\n",
        ")\n",
        "\n",
        "# ACTIVE jobs – Department\n",
        "add_result(\n",
        "    results,\n",
        "    model_name=\"bag-of-words/TF-IDF + Logistic Regression\",\n",
        "    target=\"Department (ACTIVE Jobs)\",\n",
        "    metrics={\n",
        "        \"accuracy\": d_eval_accuracy,\n",
        "        \"macro_f1\": d_eval_macro_f1\n",
        "    }\n",
        ")\n",
        "\n",
        "save_results(results)\n",
        "\n",
        "results_df_tfidf = pd.DataFrame(results)\n",
        "results_df_tfidf\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "XQKQyuc_wHv6",
        "outputId": "bd4f268d-0ab3-470c-ff15-643a7ac6b2a0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nresults = []\\n\\n# ACTIVE jobs – Seniority\\nadd_result(\\n    results,\\n    model_name=\"bag-of-words/TF-IDF + Logistic Regression\",\\n    target=\"Seniority (ACTIVE Jobs)\",\\n    metrics={\\n        \"accuracy\": s_eval_accuracy,\\n        \"macro_f1\": s_eval_macro_f1\\n    }\\n)\\n\\n# ACTIVE jobs – Department\\nadd_result(\\n    results,\\n    model_name=\"bag-of-words/TF-IDF + Logistic Regression\",\\n    target=\"Department (ACTIVE Jobs)\",\\n    metrics={\\n        \"accuracy\": d_eval_accuracy,\\n        \"macro_f1\": d_eval_macro_f1\\n    }\\n)\\n\\nsave_results(results)\\n\\nresults_df_tfidf = pd.DataFrame(results)\\nresults_df_tfidf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}